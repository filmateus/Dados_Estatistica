---
title: "Untitled"
format: html
---

```{python}
import numpy as np
```


```{python}
cimport numpy as np

class RRN():
    
    # Activation functions
    def tanh_activation(self, x):
        # tanh activation
        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
    
    def tanh_derivative(self, x):
        # Derivative of tanh
        return 1 - np.tanh(x) ** 2

    def softmax_activation(self, Z):
        # Softmax activation
        e_x = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # Numerical stability fix
        return e_x / e_x.sum(axis=0, keepdims=True)

    def mse_loss(self, y_true, y_pred):
        # Mean Squared Error Loss
        return np.mean((y_true - y_pred) ** 2)

    def __init__(self, input_size, hidden_size, output_size):
        # Initialize the parameters
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Weight matrices with small random values
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights
        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights

        # Bias vectors
        self.bh = np.zeros((hidden_size, 1))  # Hidden bias
        self.by = np.zeros((output_size, 1))  # Output bias

    def forward(self, X):
        """
        Perform a forward pass through the network
        
        X: Input sequence (shape: [sequence_length, input_size])
        """
        self.x = X
        self.h = np.zeros((self.hidden_size, 1))  # Initialize hidden state
        self.y = []  # Initialize list to store outputs

        # Loop through each time step in the input sequence
        for t in range(X.shape[0]):
            # Compute hidden state
            self.h = self.tanh_activation(np.dot(self.Wxh, self.x[t].reshape(-1, 1)) + np.dot(self.Whh, self.h) + self.bh)
            
            # Compute output
            y_t = np.dot(self.Why, self.h) + self.by
            self.y.append(y_t)
        
        # Convert the list of outputs to a numpy array
        self.y = np.array(self.y)
        
        # Apply softmax activation to each time step's output (for classification tasks)
        self.y = self.softmax_activation(self.y.T).T
        
        return self.y

    def backward(self, X, y_true, learning_rate=0.01):
        """
        Backpropagation through time (BPTT)
        
        X: Input sequence (shape: [sequence_length, input_size])
        y_true: True labels (shape: [sequence_length, output_size])
        """
        # Initialize gradients
        dWhy = np.zeros_like(self.Why)
        dby = np.zeros_like(self.by)
        dWxh = np.zeros_like(self.Wxh)
        dWhh = np.zeros_like(self.Whh)
        dbh = np.zeros_like(self.bh)

        # Compute the gradient of the loss with respect to the output
        dL_dy = self.y - y_true  # assuming MSE loss for simplicity
        
        # Backpropagate through each time step (BPTT)
        dh_next = np.zeros_like(self.h)
        
        for t in reversed(range(X.shape[0])):
            # Compute gradient for the current time step
            dWhy += np.dot(dL_dy[t], self.h.T)
            dby += dL_dy[t]
            
            # Backpropagate the error to the hidden layer
            dh = np.dot(self.Why.T, dL_dy[t]) + dh_next
            dh_raw = dh * self.tanh_derivative(self.h)  # Apply the derivative of tanh

            dWxh += np.dot(dh_raw, X[t].reshape(1, -1))  # Gradient with respect to Wxh
            dWhh += np.dot(dh_raw, self.h.T)  # Gradient with respect to Whh
            dbh += dh_raw  # Gradient with respect to bh
            
            # Update dh_next for the next iteration
            dh_next = np.dot(self.Whh.T, dh_raw)
        
        # Update parameters using gradients
        self.Wxh -= learning_rate * dWxh
        self.Whh -= learning_rate * dWhh
        self.Why -= learning_rate * dWhy
        self.bh -= learning_rate * dbh
        self.by -= learning_rate * dby


```