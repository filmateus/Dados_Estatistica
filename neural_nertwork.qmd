---
title: "Redes Neurais"
format: html
---

# Resumo de Rede Neurais

Material criado durante o estudo de Rede Neurais

## Método Backpropagation

Ou retropropagação é um algoritmo base das redes neurais que permite otimizar os pesos da rede para reduzir a funação perda, de forma a ajustar os parâmetros da rede a partir do erro obtido na saída. O **Gradiente Descendente** é utilizado para ajustar os pesos de forma eficiente. 

### Passos
    
- Propagação para frente (Forward Progagations):
    - Os dados passam pelas camadas para gerar saída.
    - A cada camada, o valor da saída é calculado usando a somada ponderada das entradas, acrecentando o viés e aplicando a função de ativação.
    - Na saída final, é comparado o valor verdadeiro e calcula-se a função perda. (EQM ou entropia cruzada, por exemplo)

$$Erro = y_{pred} - y_{true}$$

- Cálculo do erro(Gradiente)
    - Após o cálculo, o erro é propadado para trás pela rede.
    - O erro é calculado pela derivada da função perda em relação à saída da rede.
    - O algoritmo usa **regra da cadeia** para calcular o gradiente da função de perda em relação a cada pesos da rede. 

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial_{pred}} \frac{\partial_{pred}}{\partial z} \frac{\partial z}{\partial w}$$

- Propagação para trás (Backward Propagation)
    - Gradientes: o gradiente da função de perda é calculado para cada peso e viés e indica a direção e a magnitude de ajuste para minimizar a função perda.
    - Regra da cadeia: É usada para calcular como a função perda muda em relação ao pesos das camadas anteriores, ou seja, o erro é distribuído entre as camadas.

- Ajuste dos Pesos (Atualização dos Pesos)
    - Depois de calculados os gradientes, o pesos são ajustados de acordo com o gradiente descendente 
    - A atualizaçã dos pesos é feita do seguinte modo:

$$W_{novo} = W_{velho} - \eta \frac{\partial L}{\partial W}$$

- Onde:
    - $W_{novo}$ são os pesos antes da atualização,
    - $W_{velho}$ são os pesos atualizados,
    - $\eta$ é a taxa de aprendizado (um valor pequeno para garantir que os pesos não mudem muito rápido),
    - $\frac{\partial L}{\partial W}$  é o gradiente da função de perda em relação ao peso $W$.

- Interação:
    - O processo de propagação segue para frente com os novos valores, após o calculo do erro, ocorre a propagação para trás e obtidos novos valores para os pesos, sendo um processo repetido em várias intereção (epochs). A cada intereção, a tendência é diminuir o erro.

# Rede Neural Feedforwad

## Rede Neural Criada

```{python}
import numpy as np

class NeuralNetwork:

    # Activation function
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    # Derivative of the sigmoid function for backpropagation
    def sigmoid_gradient(self, x):
        return x * (1 - x)

    # Mean squared error function
    def mean_squared_error(self, y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)

    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        # Initialize random weights
        self.weights_hidden_input = np.random.randn(input_size, hidden_size)
        self.weights_hidden_output = np.random.randn(hidden_size, output_size)
        
        # Biases for each layer
        self.bias_hidden = np.random.randn(hidden_size)
        self.bias_output = np.random.randn(output_size)

        # Learning rate
        self.learning_rate = learning_rate

    def FeedForward(self, X):
        # Forward propagation
        self.hidden_input = np.dot(X, self.weights_hidden_input) + self.bias_hidden
        self.hidden_output = self.sigmoid(self.hidden_input)

        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.output = self.sigmoid(self.output_input)

        return self.output

    def BackPropagation(self, X, y):
        # Calculate the output error
        output_error = y - self.output
        output_delta = output_error * self.sigmoid_gradient(self.output)

        # Calculate the hidden layer error
        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * self.sigmoid_gradient(self.hidden_output)

        # Update weights and biases (using Gradient Descent)
        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate
        self.bias_output += np.sum(output_delta, axis=0) * self.learning_rate

        self.weights_hidden_input += X.T.dot(hidden_delta) * self.learning_rate
        self.bias_hidden += np.sum(hidden_delta, axis=0) * self.learning_rate

    def train(self, X, y, epochs=1000):
        for epoch in range(epochs):
            # Forward pass
            self.FeedForward(X)
            # Backpropagation
            self.BackPropagation(X, y)
            # Optional: display the error
            if epoch % 100 == 0:
                loss = self.mean_squared_error(y, self.output)
                print(f"Epoch {epoch}, Error: {np.round(loss, 5)}")
```

### Teste

```{python}
# Example dataset
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data for XOR
y = np.array([[0], [1], [1], [0]])               # Expected output for XOR

# Initialize network
nn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1, learning_rate=0.1)

# Train network
nn.train(X, y, epochs=1000)

# Test network
output = nn.FeedForward(X)
print("Predicted output:", output)
```

## Código em Pytorch

- Vetor de entradas

$$
X = \begin{bmatrix}
    x_{1} \\
    x_{2} \\
    \vdots \\
    x_{784}
\end{bmatrix}
$$

- Da camada de entrada para primeira camada escondida:
    - Uma camada de pesos $W_{1}$ com dimensão 128 x 784
    - Um vetor de viés(bias) $b_{1}$ de dimensão 128 x 1
    - Saída: $H_{1} = W_{1} X + b_{1}$

- Primeira para Segunda Camada Escondida:
    - Uma camada de pesos $W_{2}$ com dimensão 128 x 128
    - Um vetor de viés(bias) $b_{2}$ de dimensão 128 x 1
    - Saída: $H_{2} = W_{2} H_{1} + b_{2}$

- Segunda Camada Escondida para a Saída:
    - Uma camada de pesos $W_{3}$ com dimensão 10 x 128
    - Viés(bias) $b_{2}$ de dimensão 10 x 1
    - Saída: $Y= W_{2} H_{2} + b_{3}$


```{python}
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torch.nn.functional as F
```

```{python}
# Define the model (as in your code)
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # First layer
        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Second layer
        self.fc3 = nn.Linear(hidden_size, output_size)  # Output layer

    def forward(self, x):
        x = F.relu(self.fc1(x))  # ReLU after first layer
        x = F.relu(self.fc2(x))  # ReLU after second layer
        x = self.fc3(x)          # Output (logits)
        return x

# Hyperparameters
input_size = 784
hidden_size = 128
output_size = 10

# Initialize the model, loss function, and optimizer
model = SimpleNN(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = 0.001)

# Dataset and DataLoader (MNIST dataset)
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert images to PyTorch tensors
    transforms.Lambda(lambda x: x.view(-1))  # Flatten images (28x28 -> 784)
])
```

### Teste

```{python}
# Load the training and test data
train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

# Training function
def train(model, data_loader, criterion, optimizer, num_epochs=5):
    for epoch in range(num_epochs):
        for inputs, labels in data_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

# Train the model
train(model, train_loader, criterion, optimizer, num_epochs=5)
```